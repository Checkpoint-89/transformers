{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Salut', 'comment', 'ca', 'va', '?']]\n",
      "[['<START>', 'Hi', 'how', 'are', 'you', '?']]\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = [[\n",
    "    \"Salut\", \"comment\", \"ca\", \"va\", \"?\",\n",
    "]]\n",
    "\n",
    "output_embeddings = [[\n",
    "    \"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\",\n",
    "]]\n",
    "print(input_embeddings)\n",
    "print(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD>': 7}\n",
      "{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 6, '<PAD>': 7}\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(sequences):\n",
    "\n",
    "    token_to_info = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for word in sequence:\n",
    "            if word not in token_to_info:\n",
    "                token_to_info[word] = len(token_to_info)\n",
    "    return token_to_info\n",
    "\n",
    "input_voc = get_vocabulary(input_embeddings)\n",
    "output_voc = get_vocabulary(output_embeddings)\n",
    "\n",
    "input_voc[\"<START>\"] = len(input_voc)\n",
    "input_voc[\"<END>\"] = len(input_voc)\n",
    "input_voc[\"<PAD>\"] = len(input_voc)\n",
    "\n",
    "output_voc[\"<END>\"] = len(output_voc)\n",
    "output_voc[\"<PAD>\"] = len(output_voc)\n",
    "\n",
    "print(input_voc)\n",
    "print(output_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]]\n",
      "[[0 1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_int(sequences, voc):\n",
    "    for sequence in sequences:\n",
    "        for s, word in enumerate(sequence):\n",
    "            sequence[s] = voc[word]\n",
    "    return(np.array(sequences))\n",
    "\n",
    "input_seq = sequences_to_int(input_embeddings, input_voc)\n",
    "output_seq = sequences_to_int(output_embeddings, output_voc)\n",
    "\n",
    "print(input_seq)\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_token, **kwargs):\n",
    "        self.nb_token = nb_token\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.word_embedding = tf.keras.layers.Embedding(\n",
    "            self.nb_token, 256,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        embed = self.word_embedding(x)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "        softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        # print(\"Shape Q\", Q.shape)\n",
    "        # print(\"Shape K\", K.shape)\n",
    "        # print(\"Shape V\", V.shape)\n",
    "        # print(\"Shape QK\", QK.shape)\n",
    "        # print(\"Shape softmax\", softmax_QK.shape)\n",
    "        # print(\"Shape attention\", attention.shape)\n",
    "        return attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    attention = ScaledDotProductAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, attention)\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_51\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_94 (InputLayer)          [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer_93 (EmbeddingL  (None, 6, 256)      1536        ['input_94[0][0]']               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 6, 256)      263168      ['embedding_layer_93[0][0]',     \n",
      " HeadAttention)                                                   'embedding_layer_93[0][0]',     \n",
      "                                                                  'embedding_layer_93[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 264,704\n",
      "Trainable params: 264,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 6, 256)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dim=256, nb_head=8, **kwargs):\n",
    "        self.dim = 256\n",
    "        self.head_dim = 256 // 8\n",
    "        self.nb_head = nb_head\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        self.out_proj = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def mask_softmax(self, x, mask):\n",
    "        x_expe = tf.math.exp(x)\n",
    "        x_expe_masked = x_expe * mask\n",
    "        x_expe_sum = tf.reduce_sum(x_expe_masked, axis = -1)\n",
    "        x_expe_sum = tf.expand_dims(x_expe_sum, axis=-1)\n",
    "        softmax = x_expe_masked / x_expe_sum\n",
    "        return softmax\n",
    "\n",
    "    def call(self, x, mask = None):\n",
    "\n",
    "        in_query, in_key, in_value = x\n",
    "\n",
    "        Q = self.query_layer(in_query)\n",
    "        K = self.key_layer(in_key)\n",
    "        V = self.value_layer(in_value)\n",
    "\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_seq_len = tf.shape(Q)[1]\n",
    "        K_seq_len = tf.shape(K)[1]\n",
    "        V_seq_len = tf.shape(V)[1]\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size, Q_seq_len, self.nb_head, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, K_seq_len, self.nb_head, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, V_seq_len, self.nb_head, self.head_dim])\n",
    "\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size * self.nb_head, Q_seq_len, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size * self.nb_head, K_seq_len, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size * self.nb_head, V_seq_len, self.head_dim])\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "\n",
    "        if mask is not None:\n",
    "            QK = QK * mask\n",
    "            softmax_QK = self.mask_softmax(QK, mask)\n",
    "        else:\n",
    "            softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        attention = tf.reshape(attention, [batch_size, self.nb_head, Q_seq_len, self.head_dim])\n",
    "        attention = tf.transpose(attention, [0, 2, 1, 3])\n",
    "\n",
    "        # Concat\n",
    "        attention = tf.reshape(attention, [batch_size, Q_seq_len, self.nb_head * self.head_dim])\n",
    "\n",
    "        out_attention = self.out_proj(attention)\n",
    "\n",
    "        return out_attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(6))\n",
    "    embedding = EmbeddingLayer(nb_token=6)(layer_input)\n",
    "\n",
    "    mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "    multi_attention = MultiHeadAttention()((embedding,embedding, embedding), mask =mask)\n",
    "\n",
    "    model = tf.keras.Model(layer_input, multi_attention)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(output_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_95 (InputLayer)       [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_94 (Embeddi  (None, 5, 256)           1280      \n",
      " ngLayer)                                                        \n",
      "                                                                 \n",
      " encoder_30 (Encoder)        (None, 5, 256)            1976832   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,978,112\n",
      "Trainable params: 1,978,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense_out = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        attention = self.multi_head_attention((x, x, x))\n",
    "        post_attention = self.norm(x + attention)\n",
    "        x = self.dense_out(post_attention)\n",
    "        enc_output = self.norm(x + post_attention)\n",
    "        return enc_output\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_encoder, **kwargs):\n",
    "        self.nb_encoder = nb_encoder\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.encoder_layers = []\n",
    "        for nb in range(self.nb_encoder):\n",
    "            self.encoder_layers.append(\n",
    "                EncoderLayer()\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    enc_output = Encoder(nb_encoder=6)(embedding)\n",
    "    model = tf.keras.Model(layer_input, enc_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_53\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_96 (InputLayer)          [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer_95 (EmbeddingL  (None, 5, 256)      1280        ['input_96[0][0]']               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_97 (InputLayer)          [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " encoder_31 (Encoder)           (None, 5, 256)       1976832     ['embedding_layer_95[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_layer_96 (EmbeddingL  (None, 6, 256)      1536        ['input_97[0][0]']               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " decoder_27 (Decoder)           (None, 6, 256)       3555840     ['encoder_31[0][0]',             \n",
      "                                                                  'embedding_layer_96[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,535,488\n",
      "Trainable params: 5,535,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(1, 6, 256)\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head_self_attention = MultiHeadAttention()\n",
    "        self.multi_head_enc_attention = MultiHeadAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.proj_output = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_output, output_embedding, mask = x\n",
    "        self_attention = self.multi_head_self_attention((output_embedding, output_embedding, output_embedding), mask)\n",
    "        post_self_att = self.norm(output_embedding + self_attention)\n",
    "        enc_attention = self.multi_head_enc_attention((post_self_att, enc_output, enc_output)) # Pas sur de l'ordre\n",
    "        post_enc_attention = self.norm(enc_attention + post_self_att)\n",
    "        proj_out = self.proj_output(post_enc_attention)\n",
    "        dec_output = self.norm(proj_out + post_enc_attention)\n",
    "        return dec_output\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_decoder, **kwargs):\n",
    "        self.nb_decoder = nb_decoder\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.decoder_layers = []\n",
    "        for nb in range(self.nb_decoder):\n",
    "            self.decoder_layers.append(\n",
    "                DecoderLayer()\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        enc_out, output_embedding, mask = x\n",
    "        dec_output = output_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            dec_output = decoder_layer((enc_out, dec_output, mask))\n",
    "        return dec_output\n",
    "\n",
    "def test():\n",
    "    input_token = tf.keras.Input(shape=(5))\n",
    "    output_token = tf.keras.Input(shape=(6))\n",
    "\n",
    "    # Retrieve embedding\n",
    "    input_embedding = EmbeddingLayer(nb_token=5)(input_token)\n",
    "    output_embedding = EmbeddingLayer(nb_token=6)(output_token)\n",
    "\n",
    "    # Encoder\n",
    "    enc_output = Encoder(nb_encoder=6)(input_embedding)\n",
    "\n",
    "    # mask\n",
    "    mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "\n",
    "    # Decoder\n",
    "    dec_output = Decoder(nb_decoder=6)((enc_output, output_embedding, mask))\n",
    "\n",
    "    model = tf.keras.Model([input_token, output_token], dec_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test((input_seq, output_seq))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6eaf28abfc3d84aa2ba82674aa54b649d896e8708ee07ad2f05a5506d239dfa3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('transfo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
