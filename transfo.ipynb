{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Salut', 'comment', 'ca', 'va', '?']]\n",
      "[['<START>', 'Hi', 'how', 'are', 'you', '?']]\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = [[\n",
    "    \"Salut\", \"comment\", \"ca\", \"va\", \"?\",\n",
    "]]\n",
    "\n",
    "output_embeddings = [[\n",
    "    \"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\",\n",
    "]]\n",
    "print(input_embeddings)\n",
    "print(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD>': 7}\n",
      "{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 6, '<PAD>': 7}\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(sequences):\n",
    "\n",
    "    token_to_info = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for word in sequence:\n",
    "            if word not in token_to_info:\n",
    "                token_to_info[word] = len(token_to_info)\n",
    "    return token_to_info\n",
    "\n",
    "input_voc = get_vocabulary(input_embeddings)\n",
    "output_voc = get_vocabulary(output_embeddings)\n",
    "\n",
    "input_voc[\"<START>\"] = len(input_voc)\n",
    "input_voc[\"<END>\"] = len(input_voc)\n",
    "input_voc[\"<PAD>\"] = len(input_voc)\n",
    "\n",
    "output_voc[\"<END>\"] = len(output_voc)\n",
    "output_voc[\"<PAD>\"] = len(output_voc)\n",
    "\n",
    "print(input_voc)\n",
    "print(output_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]]\n",
      "[[0 1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_int(sequences, voc):\n",
    "    for sequence in sequences:\n",
    "        for s, word in enumerate(sequence):\n",
    "            sequence[s] = voc[word]\n",
    "    return(np.array(sequences))\n",
    "\n",
    "input_seq = sequences_to_int(input_embeddings, input_voc)\n",
    "output_seq = sequences_to_int(output_embeddings, output_voc)\n",
    "\n",
    "print(input_seq)\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_34 (Embeddi  (None, 5, 256)           1280      \n",
      " ngLayer)                                                        \n",
      "                                                                 \n",
      " scaled_dot_product_attentio  (None, 5, 256)           197376    \n",
      " n_34 (ScaledDotProductAtten                                     \n",
      " tion)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198,656\n",
      "Trainable params: 198,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_token, **kwargs):\n",
    "        self.nb_token = nb_token\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.word_embedding = tf.keras.layers.Embedding(\n",
    "            self.nb_token, 256,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        embed = self.word_embedding(x)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "        softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        return attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    attention = ScaledDotProductAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, attention)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_44 (InputLayer)       [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_43 (Embeddi  (None, 5, 256)           1280      \n",
      " ngLayer)                                                        \n",
      "                                                                 \n",
      " encoder_layer_8 (EncoderLay  (None, 5, 256)           263680    \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,960\n",
      "Trainable params: 264,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense_out = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        attention = self.scaled_dot_product_attention(x)\n",
    "        post_attention = self.norm(x + attention)\n",
    "        x = self.dense_out(post_attention)\n",
    "        enc_output = self.norm(x + post_attention)\n",
    "        return enc_output\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    enc_output = EncoderLayer()(embedding)\n",
    "    model = tf.keras.Model(layer_input, enc_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dim 32\n",
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_48 (InputLayer)       [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_47 (Embeddi  (None, 5, 256)           1280      \n",
      " ngLayer)                                                        \n",
      "                                                                 \n",
      " multi_head_attention (Multi  (None, 5, 256)           0         \n",
      " HeadAttention)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280\n",
      "Trainable params: 1,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dim=256, nb_head=8, **kwargs):\n",
    "        self.head_dim = 256 // 8\n",
    "        print(\"head_dim\", self.head_dim)\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x\n",
    "        return attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    multi_attention = MultiHeadAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, multi_attention)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "355b7f641d0ac9fedc39d758cb453bf3cda9ef564aa4dccfec403037925d63d5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('transfo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
