{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Salut', 'comment', 'ca', 'va', '?']]\n",
      "[['<START>', 'Hi', 'how', 'are', 'you', '?']]\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = [[\n",
    "    \"Salut\", \"comment\", \"ca\", \"va\", \"?\",\n",
    "]]\n",
    "\n",
    "output_embeddings = [[\n",
    "    \"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\",\n",
    "]]\n",
    "print(input_embeddings)\n",
    "print(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD>': 7}\n",
      "{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 6, '<PAD>': 7}\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(sequences):\n",
    "\n",
    "    token_to_info = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for word in sequence:\n",
    "            if word not in token_to_info:\n",
    "                token_to_info[word] = len(token_to_info)\n",
    "    return token_to_info\n",
    "\n",
    "input_voc = get_vocabulary(input_embeddings)\n",
    "output_voc = get_vocabulary(output_embeddings)\n",
    "\n",
    "input_voc[\"<START>\"] = len(input_voc)\n",
    "input_voc[\"<END>\"] = len(input_voc)\n",
    "input_voc[\"<PAD>\"] = len(input_voc)\n",
    "\n",
    "output_voc[\"<END>\"] = len(output_voc)\n",
    "output_voc[\"<PAD>\"] = len(output_voc)\n",
    "\n",
    "print(input_voc)\n",
    "print(output_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]]\n",
      "[[0 1 2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "def sequences_to_int(sequences, voc):\n",
    "    for sequence in sequences:\n",
    "        for s, word in enumerate(sequence):\n",
    "            sequence[s] = voc[word]\n",
    "    return(np.array(sequences))\n",
    "\n",
    "input_seq = sequences_to_int(input_embeddings, input_voc)\n",
    "output_seq = sequences_to_int(output_embeddings, output_voc)\n",
    "\n",
    "print(input_seq)\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_token, **kwargs):\n",
    "        self.nb_token = nb_token\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.word_embedding = tf.keras.layers.Embedding(\n",
    "            self.nb_token, 256,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        embed = self.word_embedding(x)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "        softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        # print(\"Shape Q\", Q.shape)\n",
    "        # print(\"Shape K\", K.shape)\n",
    "        # print(\"Shape V\", V.shape)\n",
    "        # print(\"Shape QK\", QK.shape)\n",
    "        # print(\"Shape softmax\", softmax_QK.shape)\n",
    "        # print(\"Shape attention\", attention.shape)\n",
    "        return attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    attention = ScaledDotProductAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, attention)\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_2 (Embeddin  (None, 5, 256)           1280      \n",
      " gLayer)                                                         \n",
      "                                                                 \n",
      " multi_head_attention (Multi  (None, 5, 256)           263168    \n",
      " HeadAttention)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,448\n",
      "Trainable params: 264,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dim=256, nb_head=8, **kwargs):\n",
    "        self.head_dim = 256 // 8\n",
    "        self.nb_head = nb_head\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        self.out_proj = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        seq_len = tf.shape(Q)[1]\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size, seq_len, self.nb_head, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, seq_len, self.nb_head, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, seq_len, self.nb_head, self.head_dim])\n",
    "\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size * self.nb_head, seq_len, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size * self.nb_head, seq_len, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size * self.nb_head, seq_len, self.head_dim])\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "        softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "\n",
    "        attention = tf.reshape(attention, [batch_size, self.nb_head, seq_len, self.head_dim])\n",
    "\n",
    "        attention = tf.transpose(attention, [0, 2, 1, 3])\n",
    "\n",
    "        # Concat\n",
    "        attention = tf.reshape(attention, [batch_size, seq_len, self.nb_head * self.head_dim])\n",
    "\n",
    "        out_attention = self.out_proj(attention)\n",
    "\n",
    "        return out_attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    multi_attention = MultiHeadAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, multi_attention)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " embedding_layer_4 (Embeddin  (None, 5, 256)           1280      \n",
      " gLayer)                                                         \n",
      "                                                                 \n",
      " encoder_layer_2 (EncoderLay  (None, 5, 256)           329472    \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330,752\n",
      "Trainable params: 330,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense_out = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        attention = self.multi_head_attention(x)\n",
    "        post_attention = self.norm(x + attention)\n",
    "        x = self.dense_out(post_attention)\n",
    "        enc_output = self.norm(x + post_attention)\n",
    "        return enc_output\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    enc_output = EncoderLayer()(embedding)\n",
    "    model = tf.keras.Model(layer_input, enc_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6eaf28abfc3d84aa2ba82674aa54b649d896e8708ee07ad2f05a5506d239dfa3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('transfo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
